{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Qq_WhunlulpF",
   "metadata": {
    "id": "Qq_WhunlulpF"
   },
   "source": [
    "# Social Computing/Social Gaming - Summer 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761nT3Fpu4sH",
   "metadata": {
    "id": "761nT3Fpu4sH"
   },
   "source": [
    "# Exercise Sheet 5: Climate Change & NLP\n",
    "Climate change dominates political debattes, already dramatically affects environments all over the world and will be biggest and most complex challenges in the years to come for humanity.\n",
    "The goal of this exercise is to analyse the toxicity/controversy of different subtopics of climate change. In order to do so, we will take a look at topic models, text classification and sentiment analysis.\n",
    "This notebook will guide you through this exercise and explain the different concepts and techniques in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118d79f",
   "metadata": {},
   "source": [
    "Execute the following code cell to install the needed libraries for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b19be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in c:\\users\\usertest\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (4.64.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (5.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (1.21.5)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (0.5.3)\n",
      "Requirement already satisfied: pyyaml<6.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (5.4.1)\n",
      "Requirement already satisfied: hdbscan>=0.8.28 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from bertopic) (0.8.28)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.7.3)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.8.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
      "Requirement already satisfied: torchvision in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.13.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.20.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: requests in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.3.15)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.7)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (61.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\usertest\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\usertest\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U tensorflow-text==2.7.3\n",
    "!pip install -q tf-models-official==2.7.0\n",
    "!pip install bertopic\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ccb5e1",
   "metadata": {
    "id": "b8ccb5e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\usertest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\usertest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "import gensim.downloader\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import sklearn\n",
    "import gensim\n",
    "import string\n",
    "import scipy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from official.nlp import optimization\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w0lJbvoa1HZW",
   "metadata": {
    "id": "w0lJbvoa1HZW"
   },
   "source": [
    "## 5.0 The Data\n",
    "The dataset we will be using consists of tweets related to climate change. The tweets were pulled directly from Twitter using their own TwitterAPI. The tweets were selected by searching for different climate related hashtags, such as #climatechange, #actonclimate, #climatehoax, etc. The data used in this project was gathered around the COP26 in Glasgow and a few days prior to the US presidential election of 2020.\n",
    "\n",
    "The dataset consists of ~14,000 tweets, from which ~2000 tweets were labeled.\n",
    "\n",
    "Tweets were labled as 'Activist' if the tweet information about the negative effects of climate change, or to act on climate change. Tweets got labled as 'Denier' if the content of the tweet questions the existens of climate change. And 'Neutral' if the tweet was neither 'Activist' nor 'Denier'\n",
    "\n",
    "### Why Twitter/Tweets?\n",
    "\n",
    "The continued growth of online social networks and micro-blogging Web services, such as Twitter, enable an extensive and near real-time data source through which the analysis of hateful and antagonistic responses to “trigger” events can be undertaken. Such data affords researchers with the possibility to measure the online social mood and emotion following large-scale, disruptive, and emotive events. Twitter is a defensible and logical source of data for such analysis given that users of social media are more likely to express emotional content due to deindividuation (anonymity, lack of self-awareness in groups, disinhibition) [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IR8rgBEI2R4_",
   "metadata": {
    "id": "IR8rgBEI2R4_"
   },
   "source": [
    "### 5.0.1 Climate Data\n",
    "Before we can begin with our analysis, we have to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40a0364",
   "metadata": {
    "id": "c40a0364"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv(\"climate_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1nP0Qf1nW5",
   "metadata": {
    "id": "6a1nP0Qf1nW5"
   },
   "source": [
    "For training we will have to encode the labels as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "u4SPK9FjYTHX",
   "metadata": {
    "id": "u4SPK9FjYTHX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>law enforcement called these intimidation tact...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>without our forests we will be dragged well be...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>more pcs are getting windows 11, is yours next...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>this is what unelected @potus want 🇺🇸 on the g...</td>\n",
       "      <td>(0, 1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>“..a..notorious switch to sunderland where an ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text      label\n",
       "0           0  law enforcement called these intimidation tact...       None\n",
       "1           1  without our forests we will be dragged well be...       None\n",
       "2           2  more pcs are getting windows 11, is yours next...       None\n",
       "3           3  this is what unelected @potus want 🇺🇸 on the g...  (0, 1, 0)\n",
       "4           4  “..a..notorious switch to sunderland where an ...       None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_to_vector(label):\n",
    "    if label == \"activist\":\n",
    "        return (1,0,0)\n",
    "    elif label == \"neutral\":\n",
    "        return (0,1,0)\n",
    "    elif label == \"denier\":\n",
    "        return (0,0,1)\n",
    "\n",
    "labels = [label_to_vector(label) for label in df.label]\n",
    "\n",
    "df[\"label\"] = labels\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J8OH7YHS3F37",
   "metadata": {
    "id": "J8OH7YHS3F37"
   },
   "source": [
    "We will remove punctuation and certain reoccuring words without useful information (stopwords) to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643c02c8",
   "metadata": {
    "id": "643c02c8"
   },
   "outputs": [],
   "source": [
    "tweets = df.text\n",
    "\n",
    "custom_stopwords = [\"think\", \"thing\", \"great\", \"every\", \"really\", \"today\", \"daily\", \"like\", \"know\",\n",
    "                    \"could\", \"would\", \"also\", \"get\", \"give\", \"dont\", \"better\", \"worse\", \"next\", \"even\",\n",
    "                    \"way\", \"look\", \"another\", \"tell\", \"talk\", \"aka\", \"one\", \"guy\", \"yet\", \"haha\", \"bro\", \n",
    "                    \"nah\", \"btw\", \"lol\", \"etc\", \"actually\", \"see\", \"either\", \"going\", \"many\", \"few\", \"amp\"]\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\") + custom_stopwords)\n",
    "\n",
    "# remove hashtag symbol\n",
    "punctuation = string.punctuation[:2] + string.punctuation[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae0d97",
   "metadata": {},
   "source": [
    "**a) Cleaning the data:** Use regular expressions in `clean` to remove links (\"https\" and \"//t.co\"), usernames and punctuation. In `remove_stopwords` remove tokens of with length < 2, if they are a digit or if they are contained in the stop words.\n",
    "\n",
    "*Hint:* Use the library `re` for the regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "_DvqEq7fN6PI",
   "metadata": {
    "id": "_DvqEq7fN6PI"
   },
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = tweet.split()\n",
    "    # DONE:\n",
    "    \n",
    "    if \"@\" in tweet: #Usernames\n",
    "        for word in words:\n",
    "            if '@' in word:\n",
    "                tweet = tweet.replace(word, '')\n",
    "    if \"https\" in tweet: #Links\n",
    "        https = re.search(\"https\", tweet)\n",
    "        tweet = tweet[:https.start()]\n",
    "    if \"//t.co\" in tweet:\n",
    "        t_co = re.search(\"//t.co\", tweet)\n",
    "        tweet = tweet[:t_co.start()] \n",
    "        \n",
    "    tweet = re.sub(r'[^\\w\\s]',' ',tweet) #Ponctuation\n",
    "    tweet = re.sub(' +', ' ', tweet) #Remove extra spaces\n",
    "    return tweet\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    copy = []\n",
    "    for token in tokens:\n",
    "        if len(token) < 2 or token.isdigit() or token in stop_words:\n",
    "            copy.append(token)\n",
    "    # DONE:\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if not token in copy:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "w-GQwzpzN9Go",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "w-GQwzpzN9Go",
    "outputId": "97b8d54d-9388-48da-c88c-1c0a0f99c55c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>law enforcement called intimidation tactics pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>without forests dragged well beyond 2c world l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pcs getting windows</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>unelected want green deal paris fooled</td>\n",
       "      <td>(0, 1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>notorious switch sunderland injury plagued rod...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text      label\n",
       "0           0  law enforcement called intimidation tactics pr...       None\n",
       "1           1  without forests dragged well beyond 2c world l...       None\n",
       "2           2                                pcs getting windows       None\n",
       "3           3             unelected want green deal paris fooled  (0, 1, 0)\n",
       "4           4  notorious switch sunderland injury plagued rod...       None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweets = [word_tokenize(clean(tweet)) for tweet in tweets]\n",
    "tokenized_tweets = [remove_stopwords(tokens) for tokens in tokenized_tweets]\n",
    "\n",
    "preprocessed_tweets = [\" \".join(tokens) for tokens in tokenized_tweets]\n",
    "\n",
    "df[\"text\"] = preprocessed_tweets\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I_HTwRhM3tGp",
   "metadata": {
    "id": "I_HTwRhM3tGp"
   },
   "source": [
    "Now that our data is prepared for training we will split the data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff445107",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff445107",
    "outputId": "aa608ee8-ca92-473a-98de-8a59789b75ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (1590, 3)\n",
      "Test dataset shape: (281, 3)\n"
     ]
    }
   ],
   "source": [
    "labeled_df = df.dropna()\n",
    "\n",
    "train, test = train_test_split(labeled_df, test_size=0.15)\n",
    "\n",
    "print('Train dataset shape: {}'.format(train.shape))\n",
    "print('Test dataset shape: {}'.format(test.shape))\n",
    "\n",
    "# convert to numpy array\n",
    "train_text = np.asarray(train.text)\n",
    "train_labels = np.asarray([np.asarray(label) for label in train.label])\n",
    "\n",
    "test_text = np.asarray(test.text)\n",
    "test_labels = np.asarray([np.asarray(label) for label in test.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "syDu9qosy-9y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "syDu9qosy-9y",
    "outputId": "b3164e02-26bd-4d51-a3ef-9ee03c9528c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print out the number of entries per label\n",
    "labeled_df['label'].value_counts()[:20].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uiaeo07U0JUN",
   "metadata": {
    "id": "Uiaeo07U0JUN"
   },
   "source": [
    "### 5.0.2 Toxicity Data\n",
    "\n",
    "For prediciting the toxicity score of a tweet, we will use the Toxic Comment Classification dataset [1], containing comments from wikipedia, labeled by their level of toxicity. Before we can use it for training, we have to preprocess it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49acd51a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "49acd51a",
    "outputId": "3ba283fe-199b-4882-af62-252bfada1e89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "toxicity_df = pd.read_csv(\"toxicity_train.csv\")\n",
    "toxicity_df = toxicity_df.drop(\"id\", axis=1)\n",
    "\n",
    "# take a look at the data\n",
    "toxicity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baGaUNVT1csT",
   "metadata": {
    "id": "baGaUNVT1csT"
   },
   "source": [
    "We will again transform the labels into encodings. We will work with three levels of toxicity: not toxic, toxic and severly toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bNQPKYTBORml",
   "metadata": {
    "id": "bNQPKYTBORml"
   },
   "outputs": [],
   "source": [
    "def relabel(label):\n",
    "    # not toxic\n",
    "    if label == (-1,-1,-1,-1,-1,-1):\n",
    "        return None\n",
    "    elif label == (0,0,0,0,0,0):\n",
    "        return (1,0,0)\n",
    "    # severely toxic\n",
    "    elif label[1] or label[3] or label[5]:\n",
    "        return (0,1,0)\n",
    "    # toxic\n",
    "    elif label[0] or label[2] or label[4]:\n",
    "        return (0,0,1)\n",
    "    else:\n",
    "        return (0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "CHk1aPfF3FFF",
   "metadata": {
    "id": "CHk1aPfF3FFF"
   },
   "outputs": [],
   "source": [
    "toxicity_df[\"label\"] = [relabel((toxicity_df.toxic[i], toxicity_df.severe_toxic[i], toxicity_df.obscene[i],\n",
    "                        toxicity_df.threat[i], toxicity_df.insult[i],\n",
    "                        toxicity_df.identity_hate[i])) for i in range(len(toxicity_df.toxic))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e701776",
   "metadata": {},
   "source": [
    "We will reuse the previously defined functions for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dH5bbqlb3qXK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dH5bbqlb3qXK",
    "outputId": "c65a7e79-d0bd-45d2-be73-1abcfb526f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (135635, 3)\n",
      "Validation dataset shape: (23936, 3)\n"
     ]
    }
   ],
   "source": [
    "toxicity_df = toxicity_df.drop([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"], axis=1)\n",
    "comments = toxicity_df.comment_text\n",
    "\n",
    "tokenized_comments = [word_tokenize(clean(comment)) for comment in comments]\n",
    "tokenized_comments = [remove_stopwords(tokens) for tokens in tokenized_comments]\n",
    "\n",
    "preprocessed_comments = [\" \".join(tokens) for tokens in tokenized_comments]\n",
    "\n",
    "toxicity_df[\"text\"] = preprocessed_comments\n",
    "\n",
    "toxicity_train, toxicity_test = train_test_split(toxicity_df, test_size=0.15)\n",
    "\n",
    "print('Train dataset shape: {}'.format(toxicity_train.shape))\n",
    "print('Validation dataset shape: {}'.format(toxicity_test.shape))\n",
    "\n",
    "# convert to numpy array\n",
    "toxicity_train_text = np.asarray(toxicity_train.comment_text)\n",
    "toxicity_train_labels = np.asarray([np.asarray(label) for label in toxicity_train.label])\n",
    "\n",
    "toxicity_test_text = np.asarray(toxicity_test.comment_text)\n",
    "toxicity_test_labels = np.asarray([np.asarray(label) for label in toxicity_test.label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JDq8Alqr8zLn",
   "metadata": {
    "id": "JDq8Alqr8zLn"
   },
   "source": [
    "### 5.0.3 Final DataFrame\n",
    "This DataFrame will be used to store the information retrieved from our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "HQb3sdJr8uVz",
   "metadata": {
    "id": "HQb3sdJr8uVz"
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=[\"text\", \"topic\", \"label\", \"toxicity\"])\n",
    "final_df[\"text\"] = tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vJOEsk3J4Bg9",
   "metadata": {
    "id": "vJOEsk3J4Bg9"
   },
   "source": [
    "## 5.1 The Topic Model\n",
    "For this task we will utilze the BERTopic library. This library embeds our pre-processed tweets using the BERT model and calculates the topics using hdbscan, a hierarchical version of DBSCAN, and a version of TF-IDF. Further information about BERTopic can be found [here](https://maartengr.github.io/BERTopic/). \n",
    "\n",
    "\n",
    "**a) Train the topic model:** Use `.fit_transform` on `topic_model` with our pre-processed tweets as input and save the returned topics and probablities for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "EUdZFuZ_KeEw",
   "metadata": {
    "id": "EUdZFuZ_KeEw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd6da8a15a6474e89275b384840c0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(preprocessed_comments)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m topics, probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_comments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bertopic\\_bertopic.py:301\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m select_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model,\n\u001b[0;32m    300\u001b[0m                                           language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m--> 301\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformed documents to Embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bertopic\\_bertopic.py:2035\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[1;34m(self, documents, method, verbose)\u001b[0m\n\u001b[0;32m   2033\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39membed_words(documents, verbose)\n\u001b[0;32m   2034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2035\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2037\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong method for extracting document/word embeddings. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2038\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither choose \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as the method. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bertopic\\backend\\_base.py:69\u001b[0m, in \u001b[0;36mBaseEmbedder.embed_documents\u001b[1;34m(self, document, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m                     document: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m     57\u001b[0m                     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\" Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    matrix of embeddings\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py:63\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     50\u001b[0m           documents: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m     51\u001b[0m           verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124;03m\"\"\" Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    matrix of embeddings\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1009\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1011\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1012\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1013\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1017\u001b[0m )\n\u001b[1;32m-> 1018\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1031\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    598\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    600\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    483\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    492\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    415\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 423\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:327\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    324\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    330\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(language=\"english\", min_topic_size=12, calculate_probabilities=True, verbose=True)\n",
    "print(preprocessed_comments)\n",
    "# DONE:\n",
    "topics, probabilities = topic_model.fit_transform(preprocessed_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ABYhyYUVmsUr",
   "metadata": {
    "id": "ABYhyYUVmsUr"
   },
   "source": [
    "Now that we have trained the model, we can take a look at the extracted topics by using different visualizations provided by BERTopic.\n",
    "\n",
    "\n",
    "**b) Analyse and adjust the topics:** You can find the pre-defined functions [here](https://maartengr.github.io/BERTopic/). You can reduce the amount of topics with `.reduce_topics`, try to find a suitable number of topics for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE:\n",
    "topic_model.reduce_topics(preprocessed_comments,topics, nr_topics = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GLg78JUxM2OG",
   "metadata": {
    "id": "GLg78JUxM2OG"
   },
   "source": [
    "**TODO: Write your observations here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zF7DA6mb4gCx",
   "metadata": {
    "id": "zF7DA6mb4gCx"
   },
   "source": [
    "## 5.2 Text Classification\n",
    "In this task we will make use of the BERT model and append feed-forward layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2KqU93QN43O-",
   "metadata": {
    "id": "2KqU93QN43O-"
   },
   "outputs": [],
   "source": [
    "# small bert\n",
    "bert_model_handle = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "bert_preprocess_handle = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juLPVNWM8XWn",
   "metadata": {
    "id": "juLPVNWM8XWn"
   },
   "source": [
    "### 5.2.1\n",
    "\n",
    "In this task we will implement the function that builds our classifier.\n",
    "\n",
    "Below you can see the architecture of our model. Append the missing dense and dropout layers with `keras.layers.Dense` and `keras.layers.Dropout` respectively. For the dropout we will use a value of 0.2 and the dense layers will have 256, 64, 3 neurons.\n",
    "\n",
    "*Note:* Output corresponds to the last layer and is only present for visualization purposes.\n",
    "\n",
    "![classifier.PNG](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "uoGg_ex3T8xT",
   "metadata": {
    "id": "uoGg_ex3T8xT"
   },
   "outputs": [],
   "source": [
    "def build_classifier_bert():\n",
    "    # define the input layer\n",
    "    text_input = keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "\n",
    "    # define the preprocessing layer for the BERT model\n",
    "    preprocessing_layer = hub.KerasLayer(bert_preprocess_handle, name=\"preprocessing\")\n",
    "\n",
    "    # connect the preprocessing layer to the BERT model\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "\n",
    "    # load the BERT model from tensorflow hub\n",
    "    encoder = hub.KerasLayer(bert_model_handle, trainable=True, name=\"BERT_encoder\")\n",
    "\n",
    "    # get the encoder outputs and connect them to our feedforward part\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs[\"pooled_output\"]\n",
    "\n",
    "    # DONE:\n",
    "    dense1 = keras.layers.Dense(256)\n",
    "    dense2 = keras.layers.Dense(64)\n",
    "    dense3 = keras.layers.Dense(3)\n",
    "    dropout = keras.layers.Dropout(0.2)\n",
    "    \n",
    "    return keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kqk1dQjo6mDC",
   "metadata": {
    "id": "kqk1dQjo6mDC"
   },
   "source": [
    "### 5.2.2\n",
    "Since BERT is a rather large model we will use the GPU to speed up training.\n",
    "\n",
    "\n",
    "**a) Build the model:** Before we can train the model we have to build it using the previous defined `build_classifier_bert` function. \n",
    "\n",
    "\n",
    "**b) Prepare for trianing:** Since we want to predict one-hot encodings of our labels, we will use the `CategoricalCrossentropy` as our loss and the `CategoricalAccuracy` for our metric. You can call them directly from `keras.losses` and `keras.metrics` respectively. Additionally, we want to take a look at the precision and recall during the evaluation. Add the corresponding metrics.\n",
    "\n",
    "\n",
    "Next we will have to specify the number of epochs and training steps. You can experiment with the number of epochs until you get a satisfying result.\n",
    "\n",
    "\n",
    "Before we can compile the model, we need to initialize the optimizer. Here we will use a variation of the Adam optimization algorithm that was also used during the training of the BERT model.\n",
    "\n",
    "\n",
    "Now we can put everything together and compile our model.\n",
    "\n",
    "\n",
    "**c) Train the model:** Call the `.fit` method on the classifier, use the training set as input and set the correct amount of epochs. If you want to receive information of the training process set `verbose=1`. After the fine-tuning of our model, test it using `.evaluate` and print out the loss and metrics.\n",
    " \n",
    "\n",
    "**d) Predict the labels:** Use the model to predict the labels of our complete dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "Q4WCLelg5a9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4WCLelg5a9N",
    "outputId": "28a3476e-c59c-448f-e7c2-d9115dba9315"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m text_classifier\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     24\u001b[0m                         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m     25\u001b[0m                         metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# DONE b: start fine-tuning the model and evaluate it\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtext_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m text_classifier\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    # DONE a.1: build the classifier\n",
    "    text_classifier = build_classifier_bert()\n",
    "    \n",
    "    # DONE a.2: define the loss and metric functions\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    metrics = keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    # define epochs and number of training steps\n",
    "    epochs = 10\n",
    "    steps_per_epoch = len(train)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    # define the optimizer\n",
    "    init_lr = 3e-5\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type='adamw')\n",
    "\n",
    "    # compile the model using defined optimizer, loss and metric objects\n",
    "    text_classifier.compile(optimizer=optimizer,\n",
    "                            loss=loss,\n",
    "                            metrics=metrics)\n",
    "\n",
    "    # DONE b: start fine-tuning the model and evaluate it\n",
    "    text_classifier.fit(train, epochs = epochs)\n",
    "    text_classifier.evaluate()\n",
    "    print(loss)\n",
    "    print(metrics)\n",
    "    # DONE c: predict the labels of the climate data and add them to final_df\n",
    "    # Hint: the predictions are returned as vectors of class porbabilities;\n",
    "    # map the probabilities to the corresponding label\n",
    "    final_df['label'] = text_classifier.predict(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Dbxn9Si-zPk",
   "metadata": {
    "id": "-Dbxn9Si-zPk"
   },
   "source": [
    "## 5.3 Toxicity predictor\n",
    "For predicting the toxicity of a tweet we will use the same architecture for our classifier as in the previous task.\n",
    "\n",
    "**a) Build the classifier, train and evaluate it**\n",
    "\n",
    "*Hint:* Due to the high amount of training data, less epochs are needed to achieve a good result.\n",
    "\n",
    "**b) Use the trained classifier for predicting the toxicity labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b5a5a38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b5a5a38",
    "outputId": "4e6239e9-e45d-41e1-b22d-3161c713b554"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m classifier\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     20\u001b[0m                         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m     21\u001b[0m                         metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# DONE b: start fine-tuning the model and evaluate it\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m classifier\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# TODO b: \u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    # DONE a:\n",
    "    classifier = build_classifier_bert()\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    metrics = keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    epochs = 5\n",
    "    steps_per_epoch = len(train)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    init_lr = 3e-5\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type='adamw')\n",
    "\n",
    "    # compile the model using defined optimizer, loss and metric objects\n",
    "    classifier.compile(optimizer=optimizer,\n",
    "                            loss=loss,\n",
    "                            metrics=metrics)\n",
    "\n",
    "    # DONE b: start fine-tuning the model and evaluate it\n",
    "    classifier.fit(train, epochs = epochs)\n",
    "    classifier.evaluate()\n",
    "    # TODO b: \n",
    "    classifier.predict(final_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dsy7aTVN_7KI",
   "metadata": {
    "id": "Dsy7aTVN_7KI"
   },
   "source": [
    "## 5.4 Analysis of our data\n",
    "Now that we have all the models we need we can start our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fvFokCs6ZIwY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fvFokCs6ZIwY",
    "outputId": "bd390b64-daee-4ed3-df9e-27113964b304"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>law enforcement called these intimidation tact...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>without our forests we will be dragged well be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>more pcs are getting windows 11, is yours next...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is what unelected @potus want 🇺🇸 on the g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“..a..notorious switch to sunderland where an ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text topic label toxicity\n",
       "0  law enforcement called these intimidation tact...   NaN   NaN      NaN\n",
       "1  without our forests we will be dragged well be...   NaN   NaN      NaN\n",
       "2  more pcs are getting windows 11, is yours next...   NaN   NaN      NaN\n",
       "3  this is what unelected @potus want 🇺🇸 on the g...   NaN   NaN      NaN\n",
       "4  “..a..notorious switch to sunderland where an ...   NaN   NaN      NaN"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at our already gathered data\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aAIhGDnp4CBX",
   "metadata": {
    "id": "aAIhGDnp4CBX"
   },
   "outputs": [],
   "source": [
    "def from_vector(v):\n",
    "    if v == \"not toxic\":\n",
    "        return 0\n",
    "    elif v == \"toxic\":\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7ddf6",
   "metadata": {},
   "source": [
    "**a) Calculate the average toxicity per topic:** Iterate over all topics and calculate their average toxicity, the average toxicity per label and the share of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "IR9RF-XhZOS2",
   "metadata": {
    "id": "IR9RF-XhZOS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_activists</th>\n",
       "      <th>avg_toxicity_activists</th>\n",
       "      <th>num_neutrals</th>\n",
       "      <th>avg_toxicity_neutrals</th>\n",
       "      <th>num_deniers</th>\n",
       "      <th>avg_toxicity_deniers</th>\n",
       "      <th>avg_toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [num_activists, avg_toxicity_activists, num_neutrals, avg_toxicity_neutrals, num_deniers, avg_toxicity_deniers, avg_toxicity]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_topic = pd.DataFrame(columns=[\"num_activists\", \"avg_toxicity_activists\",\n",
    "                                 \"num_neutrals\", \"avg_toxicity_neutrals\",\n",
    "                                 \"num_deniers\", \"avg_toxicity_deniers\",\n",
    "                                 \"avg_toxicity\"])\n",
    "\n",
    "num_topics = final_df[\"topic\"].nunique()-1\n",
    "\n",
    "# TODO:\n",
    "for topic in by_topic:\n",
    "    \n",
    "by_topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c6929",
   "metadata": {},
   "source": [
    "**b) Visualize the average toxicity by topic:** Try to find good visualizations. \n",
    "Examples:\n",
    "* Use a bar plot for `by_topic`, you can also drop the share of labels per topic, to get a better overview\n",
    "* Sort topics by their toxicity and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6215f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 0 artists>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO:\n",
    "import matplotlib\n",
    "matplotlib.pyplot.bar(by_topic,num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f8b80",
   "metadata": {},
   "source": [
    "**c) Visualize the overall toxicity and by label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lYtYkWcXROlj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYtYkWcXROlj",
    "outputId": "875db1c2-e786-452b-ee7c-c3fac5d21df8"
   },
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984ff8d",
   "metadata": {},
   "source": [
    "**d)** Discuss the visualizations and try to make assumptions based on these. Which group is more toxic and why? What are the distributions among the data? Are there topics dominated by certain groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IE_y-3Kf96I7",
   "metadata": {
    "id": "IE_y-3Kf96I7"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Climate_and_NLP_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
